% Encoding: ISO-8859-1

@TechReport{Khalfaoui2018DropLasso,
  author      = {Khalfaoui, B. and Vert, J.-P.},
  title       = {DropLasso: A robust variant of Lasso for single cell {RNA}-seq data},
  institution = {HAL},
  year        = {2018},
  number      = {01716704},
  owner       = {jp},
  timestamp   = {2018.03.18},
}

@Article{Srivastava2014Dropout,
  author    = {Srivastava, N. and Hinton, G. and Krizhevsky, A. and Sutskever, I. and Salakhutdinov, R.},
  title     = {Dropout: A simple way to prevent neural networks from overfitting},
  journal   = {J. Mach. Learn. Res.},
  year      = {2014},
  volume    = {15},
  number    = {1},
  pages     = {1929--1958},
  owner     = {jp},
  publisher = {JMLR. org},
  timestamp = {2018.01.29},
}

@Article{Tibshirani1996Regression,
  author    = {Tibshirani, R.},
  title     = {Regression shrinkage and selection via the lasso},
  journal   = {J. R. Stat. Soc. Ser. B},
  year      = {1996},
  volume    = {58},
  number    = {1},
  pages     = {267--288},
  file      = {Tibshirani1996Regression.pdf:Tibshirani1996Regression.pdf:PDF},
  owner     = {jp},
  timestamp = {2018.11.04},
  url       = {http://www.jstor.org/stable/2346178},
}

@Article{Zou2005Regularization,
  author    = {Zou, H. and Hastie, T.},
  title     = {Regularization and variable selection via the {E}lastic {N}et},
  journal   = {J. R. Stat. Soc. Ser. B},
  year      = {2005},
  volume    = {67},
  pages     = {301--320},
  abstract  = {Summary. We propose the elastic net, a new regularization and variable
	selection method. Real world data and a simulation study show that
	the elastic net often outperforms the lasso, while enjoying a similar
	sparsity of representation. In addition, the elastic net encourages
	a grouping effect, where strongly correlated predictors tend to be
	in or out of the model together.The elastic net is particularly useful
	when the number of predictors (p) is much bigger than the number
	of observations (n). By contrast, the lasso is not a very satisfactory
	variable selection method in the p n case. An algorithm called LARS-EN
	is proposed for computing elastic net regularization paths efficiently,
	much like algorithm LARS does for the lasso.},
  file      = {Zou2005Regularization.pdf:Zou2005Regularization.pdf:PDF},
  keywords  = {elastic-net, feature-selection, lars, lasso},
  owner     = {jp},
  timestamp = {2018.11.04},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.1596},
}

@Comment{jabref-meta: databaseType:bibtex;}
