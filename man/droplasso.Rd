% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/droplasso.R
\name{droplasso}
\alias{droplasso}
\title{Fit a droplasso model}
\usage{
droplasso(x, y, family = c("gaussian", "binomial"), keep_prob = 0.5,
  lambda = 0, init = numeric(ncol(x)), gamma0 = 1, decay = 0.01,
  n_passes = 1000, minibatch_size = nrow(x))
}
\arguments{
\item{x}{Input matrix, of dimension \code{nobs x nvars}; each row is an
observation vector.}

\item{y}{Response variable.}

\item{family}{Response type. \code{family="gaussian"} (default) for least squares regression, \code{family="binomial"} for logistic regression}

\item{keep_prob}{The probability that each element is kept (default: \code{0.5})}

\item{lambda}{Regularisation parameter of the l_1 norm (default: \code{0})}

\item{init}{Initial model to start optimization (default: zero vector).}

\item{gamma0}{Initial value of the learning rate (default: \code{1})}

\item{decay}{Learning rate decay (default: \code{0.01})}

\item{n_passes}{Number of passes over each example of the data on average (default: \code{1000})}

\item{minibatch_size}{Batch size (default: \code{nobs})}
}
\value{
A vector of coefficients of size \code{nvars} that solves the droplasso problem. The optimization problem is solved with a stochastic proximal gradient descent algorithm, using mini-batches of size \code{minibatch_size}, and a learning rate decaying as \code{gamma0/(1+decay*t)}, where \code{t} is the number of mini-batches processed.
}
\description{
Fit a dropout lasso (droplasso) model.
}
\examples{
#create data:
nobs = 100
nvars = 5
x = matrix(rnorm(nobs*nvars),nrow=nobs)
b = c(1,1,0,0,0)
p = 1/(1+exp(-x\%*\%b))
y = p>0.5
# Fit a lasso model (no dropout)
droplasso(x, y, family="binomial", lambda=0.1, keep_prob=1)
# Fit a dropout model (no lasso)
droplasso(x, y, family="binomial", lambda=0, keep_prob=0.5)
# Fit a dropout lasso model
droplasso(x, y, family="binomial", lambda=0.1, keep_prob=0.5)
}
